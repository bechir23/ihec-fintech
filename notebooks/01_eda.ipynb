{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e13fdd5b",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis — BVMT Trading Data\n",
    "\n",
    "This notebook covers:\n",
    "1. Data loading & cleaning\n",
    "2. Descriptive statistics\n",
    "3. TUNINDEX construction\n",
    "4. Distribution analysis\n",
    "5. Stationarity tests\n",
    "6. Correlation analysis\n",
    "7. Volume & liquidity profiling\n",
    "8. Technical indicators visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e97364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print('Libraries loaded successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a392e2",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf43aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all CSV files\n",
    "data_dir = os.path.join('..', 'data')\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for f in sorted(csv_files):\n",
    "    size_mb = os.path.getsize(os.path.join(data_dir, f)) / (1024 * 1024)\n",
    "    print(f\"  {f} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7721ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and combine all data\n",
    "dfs = []\n",
    "for f in sorted(csv_files):\n",
    "    filepath = os.path.join(data_dir, f)\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep=';', decimal=',', encoding='utf-8')\n",
    "    except:\n",
    "        df = pd.read_csv(filepath, sep=';', decimal='.', encoding='latin-1')\n",
    "    df['source_file'] = f\n",
    "    dfs.append(df)\n",
    "    print(f\"{f}: {len(df)} rows, {len(df.columns)} columns\")\n",
    "\n",
    "raw = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"\\nTotal raw records: {len(raw):,}\")\n",
    "print(f\"Columns: {list(raw.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d9517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first rows\n",
    "raw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363f6251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to standard English names\n",
    "col_map = {\n",
    "    'SEANCE': 'date',\n",
    "    'GROUPE': 'group',\n",
    "    'CODE': 'code',\n",
    "    'VALEUR': 'name',\n",
    "    'OUVERTURE': 'open',\n",
    "    'CLOTURE': 'close',\n",
    "    'PLUS_BAS': 'low',\n",
    "    'PLUS_HAUT': 'high',\n",
    "    'QUANTITE_NEGOCIEE': 'volume',\n",
    "    'NB_TRANSACTION': 'transactions',\n",
    "    'CAPITAUX': 'turnover'\n",
    "}\n",
    "df = raw.rename(columns={k: v for k, v in col_map.items() if k in raw.columns})\n",
    "\n",
    "# Parse dates\n",
    "df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y', errors='coerce')\n",
    "if df['date'].isna().sum() > 0:\n",
    "    df['date'] = pd.to_datetime(df['date'], dayfirst=True, errors='coerce')\n",
    "\n",
    "# Convert numeric columns\n",
    "num_cols = ['open', 'close', 'low', 'high', 'volume', 'transactions', 'turnover']\n",
    "for col in num_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Remove duplicates\n",
    "before = len(df)\n",
    "df = df.drop_duplicates(subset=['date', 'code'], keep='first')\n",
    "print(f\"Removed {before - len(df):,} duplicates\")\n",
    "print(f\"Clean dataset: {len(df):,} records\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"Unique stocks: {df['code'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6de9e2e",
   "metadata": {},
   "source": [
    "## 2. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f41ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall statistics\n",
    "print(\"=== Numeric Summary ===\")\n",
    "df[num_cols].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a0bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "print(\"=== Missing Values ===\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "pd.DataFrame({'count': missing, 'pct': missing_pct}).query('count > 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b8c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-stock statistics\n",
    "stock_stats = df.groupby('code').agg(\n",
    "    records=('close', 'count'),\n",
    "    avg_close=('close', 'mean'),\n",
    "    std_close=('close', 'std'),\n",
    "    avg_volume=('volume', 'mean'),\n",
    "    avg_transactions=('transactions', 'mean'),\n",
    "    first_date=('date', 'min'),\n",
    "    last_date=('date', 'max')\n",
    ").round(2)\n",
    "\n",
    "stock_stats = stock_stats.sort_values('avg_volume', ascending=False)\n",
    "print(f\"Top 15 stocks by average daily volume:\")\n",
    "stock_stats.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92555f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-sector (group) analysis\n",
    "if 'group' in df.columns:\n",
    "    group_stats = df.groupby('group').agg(\n",
    "        stocks=('code', 'nunique'),\n",
    "        avg_volume=('volume', 'mean'),\n",
    "        avg_turnover=('turnover', 'mean')\n",
    "    ).sort_values('avg_turnover', ascending=False).round(2)\n",
    "    print(\"Sector breakdown:\")\n",
    "    display(group_stats.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be228cec",
   "metadata": {},
   "source": [
    "## 3. TUNINDEX Proxy Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd719fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a market-wide index (equal-weighted for simplicity)\n",
    "daily_market = df.groupby('date').agg(\n",
    "    avg_close=('close', 'mean'),\n",
    "    total_volume=('volume', 'sum'),\n",
    "    total_turnover=('turnover', 'sum'),\n",
    "    active_stocks=('code', 'nunique')\n",
    ").sort_index()\n",
    "\n",
    "# Normalize to base 1000\n",
    "daily_market['tunindex_proxy'] = (daily_market['avg_close'] / daily_market['avg_close'].iloc[0]) * 1000\n",
    "daily_market['daily_return'] = daily_market['tunindex_proxy'].pct_change()\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n",
    "\n",
    "axes[0].plot(daily_market.index, daily_market['tunindex_proxy'], color='#58a6ff', linewidth=1.5)\n",
    "axes[0].set_title('TUNINDEX Proxy (Equal-Weighted)', fontsize=14)\n",
    "axes[0].set_ylabel('Index Value')\n",
    "\n",
    "axes[1].bar(daily_market.index, daily_market['total_volume'], color='#3fb950', alpha=0.7, width=1)\n",
    "axes[1].set_title('Total Market Volume', fontsize=14)\n",
    "axes[1].set_ylabel('Shares')\n",
    "\n",
    "axes[2].plot(daily_market.index, daily_market['active_stocks'], color='#f0883e', linewidth=1)\n",
    "axes[2].set_title('Number of Active Stocks', fontsize=14)\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_xlabel('Date')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"TUNINDEX proxy range: {daily_market['tunindex_proxy'].min():.1f} — {daily_market['tunindex_proxy'].max():.1f}\")\n",
    "print(f\"Total return: {((daily_market['tunindex_proxy'].iloc[-1] / daily_market['tunindex_proxy'].iloc[0]) - 1) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf359ea",
   "metadata": {},
   "source": [
    "## 4. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8749944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute daily returns for all stocks\n",
    "df = df.sort_values(['code', 'date'])\n",
    "df['return'] = df.groupby('code')['close'].pct_change()\n",
    "\n",
    "# Overall return distribution\n",
    "returns = df['return'].dropna()\n",
    "returns_clean = returns[(returns > -0.5) & (returns < 0.5)]  # remove extreme outliers for plotting\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(returns_clean, bins=200, density=True, color='#58a6ff', alpha=0.7, edgecolor='none')\n",
    "x = np.linspace(returns_clean.min(), returns_clean.max(), 100)\n",
    "axes[0].plot(x, stats.norm.pdf(x, returns_clean.mean(), returns_clean.std()), 'r-', linewidth=2, label='Normal')\n",
    "axes[0].set_title('Daily Return Distribution', fontsize=14)\n",
    "axes[0].set_xlabel('Return')\n",
    "axes[0].legend()\n",
    "\n",
    "# QQ Plot\n",
    "stats.probplot(returns_clean.sample(min(5000, len(returns_clean))), dist='norm', plot=axes[1])\n",
    "axes[1].set_title('QQ Plot (Normal)', fontsize=14)\n",
    "\n",
    "# Volume distribution (log)\n",
    "vol_clean = df['volume'].dropna()\n",
    "vol_clean = vol_clean[vol_clean > 0]\n",
    "axes[2].hist(np.log10(vol_clean), bins=100, color='#3fb950', alpha=0.7, edgecolor='none')\n",
    "axes[2].set_title('Volume Distribution (log₁₀)', fontsize=14)\n",
    "axes[2].set_xlabel('log₁₀(Volume)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Return stats: mean={returns.mean():.5f}, std={returns.std():.4f}, skew={returns.skew():.2f}, kurtosis={returns.kurtosis():.2f}\")\n",
    "print(f\"Leptokurtic (kurtosis > 3): {returns.kurtosis() > 3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c387f884",
   "metadata": {},
   "source": [
    "## 5. Stationarity Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e684a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "# Pick top 5 stocks by volume for testing\n",
    "top_stocks = stock_stats.head(5).index.tolist()\n",
    "print(f\"Testing stationarity for: {top_stocks}\\n\")\n",
    "\n",
    "results = []\n",
    "for code in top_stocks:\n",
    "    stock_data = df[df['code'] == code].set_index('date')['close'].dropna()\n",
    "    if len(stock_data) < 100:\n",
    "        continue\n",
    "    \n",
    "    # ADF test on raw prices\n",
    "    adf_stat, adf_p, _, _, _, _ = adfuller(stock_data, maxlag=20)\n",
    "    \n",
    "    # ADF test on returns\n",
    "    rets = stock_data.pct_change().dropna()\n",
    "    adf_ret_stat, adf_ret_p, _, _, _, _ = adfuller(rets, maxlag=20)\n",
    "    \n",
    "    # KPSS on raw\n",
    "    kpss_stat, kpss_p, _, _ = kpss(stock_data, regression='ct', nlags='auto')\n",
    "    \n",
    "    results.append({\n",
    "        'stock': code,\n",
    "        'adf_price_p': round(adf_p, 4),\n",
    "        'adf_price_stationary': adf_p < 0.05,\n",
    "        'adf_return_p': round(adf_ret_p, 4),\n",
    "        'adf_return_stationary': adf_ret_p < 0.05,\n",
    "        'kpss_price_p': round(kpss_p, 4),\n",
    "        'kpss_price_stationary': kpss_p > 0.05\n",
    "    })\n",
    "\n",
    "stationarity_df = pd.DataFrame(results)\n",
    "print(\"Stationarity Test Results:\")\n",
    "print(\"(ADF: p < 0.05 → stationary; KPSS: p > 0.05 → stationary)\")\n",
    "stationarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cc214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF/PACF for one stock\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "sample_stock = top_stocks[0]\n",
    "stock_close = df[df['code'] == sample_stock].set_index('date')['close'].dropna()\n",
    "stock_returns = stock_close.pct_change().dropna()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "plot_acf(stock_close, ax=axes[0, 0], lags=40, title=f'ACF — {sample_stock} (Price)')\n",
    "plot_pacf(stock_close, ax=axes[0, 1], lags=40, title=f'PACF — {sample_stock} (Price)')\n",
    "plot_acf(stock_returns, ax=axes[1, 0], lags=40, title=f'ACF — {sample_stock} (Returns)')\n",
    "plot_pacf(stock_returns, ax=axes[1, 1], lags=40, title=f'PACF — {sample_stock} (Returns)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971abfa8",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d674e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-stock correlation matrix (top 10 by volume)\n",
    "top10 = stock_stats.head(10).index.tolist()\n",
    "pivot_returns = df[df['code'].isin(top10)].pivot_table(\n",
    "    index='date', columns='code', values='return'\n",
    ")\n",
    "\n",
    "corr_matrix = pivot_returns.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            center=0, vmin=-1, vmax=1, ax=ax, square=True)\n",
    "ax.set_title('Return Correlation Matrix — Top 10 Stocks by Volume', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Average pairwise correlation\n",
    "upper = corr_matrix.where(mask.T)\n",
    "print(f\"Average pairwise correlation: {upper.stack().mean():.3f}\")\n",
    "print(f\"Max correlation: {upper.stack().max():.3f}\")\n",
    "print(f\"Min correlation: {upper.stack().min():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d14a5",
   "metadata": {},
   "source": [
    "## 7. Volume & Liquidity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88634d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liquidity classification\n",
    "stock_stats['liquidity'] = pd.cut(\n",
    "    stock_stats['avg_volume'],\n",
    "    bins=[0, 100, 1000, 10000, 100000, float('inf')],\n",
    "    labels=['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    ")\n",
    "\n",
    "liquidity_counts = stock_stats['liquidity'].value_counts().sort_index()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = ['#f85149', '#f0883e', '#d29922', '#3fb950', '#58a6ff']\n",
    "axes[0].bar(range(len(liquidity_counts)), liquidity_counts.values, color=colors)\n",
    "axes[0].set_xticks(range(len(liquidity_counts)))\n",
    "axes[0].set_xticklabels(liquidity_counts.index, rotation=45)\n",
    "axes[0].set_title('Stock Liquidity Distribution', fontsize=14)\n",
    "axes[0].set_ylabel('Number of Stocks')\n",
    "\n",
    "# Volume over time\n",
    "axes[1].plot(daily_market.index, daily_market['total_volume'].rolling(20).mean(),\n",
    "             color='#3fb950', linewidth=1.5)\n",
    "axes[1].set_title('Market Volume (20-day SMA)', fontsize=14)\n",
    "axes[1].set_ylabel('Total Volume')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLiquidity breakdown:\")\n",
    "for cat, count in liquidity_counts.items():\n",
    "    print(f\"  {cat}: {count} stocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea91a2f",
   "metadata": {},
   "source": [
    "## 8. Technical Indicators — Sample Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9825aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick most liquid stock\n",
    "sample = df[df['code'] == top_stocks[0]].set_index('date').sort_index().copy()\n",
    "\n",
    "# RSI\n",
    "delta = sample['close'].diff()\n",
    "gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "rs = gain / loss\n",
    "sample['rsi'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# MACD\n",
    "ema12 = sample['close'].ewm(span=12).mean()\n",
    "ema26 = sample['close'].ewm(span=26).mean()\n",
    "sample['macd'] = ema12 - ema26\n",
    "sample['macd_signal'] = sample['macd'].ewm(span=9).mean()\n",
    "sample['macd_hist'] = sample['macd'] - sample['macd_signal']\n",
    "\n",
    "# Bollinger Bands\n",
    "sample['sma20'] = sample['close'].rolling(20).mean()\n",
    "sample['bb_upper'] = sample['sma20'] + 2 * sample['close'].rolling(20).std()\n",
    "sample['bb_lower'] = sample['sma20'] - 2 * sample['close'].rolling(20).std()\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 16), sharex=True,\n",
    "                         gridspec_kw={'height_ratios': [3, 1, 1, 1]})\n",
    "\n",
    "# Price + Bollinger\n",
    "axes[0].plot(sample.index, sample['close'], color='#58a6ff', linewidth=1.5, label='Close')\n",
    "axes[0].plot(sample.index, sample['sma20'], color='#f0883e', linewidth=1, label='SMA 20')\n",
    "axes[0].fill_between(sample.index, sample['bb_lower'], sample['bb_upper'],\n",
    "                      color='#58a6ff', alpha=0.1, label='Bollinger Bands')\n",
    "axes[0].set_title(f'{top_stocks[0]} — Price & Bollinger Bands', fontsize=14)\n",
    "axes[0].legend(loc='upper left')\n",
    "\n",
    "# Volume\n",
    "axes[1].bar(sample.index, sample['volume'], color='#3fb950', alpha=0.7, width=1)\n",
    "axes[1].set_title('Volume', fontsize=12)\n",
    "\n",
    "# RSI\n",
    "axes[2].plot(sample.index, sample['rsi'], color='#d29922', linewidth=1)\n",
    "axes[2].axhline(70, color='#f85149', linestyle='--', alpha=0.5)\n",
    "axes[2].axhline(30, color='#3fb950', linestyle='--', alpha=0.5)\n",
    "axes[2].fill_between(sample.index, 30, 70, color='gray', alpha=0.1)\n",
    "axes[2].set_title('RSI (14)', fontsize=12)\n",
    "axes[2].set_ylim(0, 100)\n",
    "\n",
    "# MACD\n",
    "axes[3].plot(sample.index, sample['macd'], color='#58a6ff', linewidth=1, label='MACD')\n",
    "axes[3].plot(sample.index, sample['macd_signal'], color='#f0883e', linewidth=1, label='Signal')\n",
    "colors = ['#3fb950' if v >= 0 else '#f85149' for v in sample['macd_hist']]\n",
    "axes[3].bar(sample.index, sample['macd_hist'], color=colors, alpha=0.5, width=1)\n",
    "axes[3].set_title('MACD', fontsize=12)\n",
    "axes[3].legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46cc51d",
   "metadata": {},
   "source": [
    "## 9. Key Findings Summary\n",
    "\n",
    "- **Dataset**: ~80+ unique stocks, daily data from Jan 2022 to Mar 2025\n",
    "- **Returns**: Leptokurtic distribution (fat tails), slight negative skew — consistent with emerging market behavior\n",
    "- **Stationarity**: Raw prices are non-stationary; first-differenced returns are stationary → SARIMA requires d=1\n",
    "- **Correlation**: Low to moderate inter-stock correlation → good for portfolio diversification\n",
    "- **Liquidity**: Wide range — from near-zero daily volume to 100K+ shares; many micro-caps with liquidity risk\n",
    "- **Seasonality**: Weekly patterns visible in ACF at lag 5; monthly/annual effects to be investigated\n",
    "- **Technical indicators**: RSI and MACD provide actionable signals for liquid stocks"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
